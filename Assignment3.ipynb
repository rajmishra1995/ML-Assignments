{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "792gPgwZAgGb"
      },
      "outputs": [],
      "source": [
        "import random as rd\n",
        "import re\n",
        "import math\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "8ewpKFmEA0j_"
      },
      "outputs": [],
      "source": [
        "class K_Means_Class:\n",
        "\n",
        "    def __init__(self, strURL, k = 5, tolerance = 0.001, maxIter = 10, data_table = []):\n",
        "        self.k = k\n",
        "        self.tolerance = tolerance\n",
        "        self.maxIter = maxIter\n",
        "        self.strURL = strURL\n",
        "        #self.data_table = data_table\n",
        "\n",
        "\n",
        "    def pre_process_raw_data(self):\n",
        "        \n",
        "        r = requests.get(self.strURL, allow_redirects=True)\n",
        "\n",
        "        open('dataset.txt', 'wb').write(r.content)\n",
        "        dataFile = open('/content/dataset.txt', \"r\", encoding=\"utf8\")  \n",
        "\n",
        "        tweetsData = list(dataFile)\n",
        "        tweets_array = []\n",
        "\n",
        "        for i in range(len(tweetsData)):\n",
        "\n",
        "            # remove \\n from the end after every sentence\n",
        "            tweetsData[i] = tweetsData[i].strip('\\n')\n",
        "\n",
        "            # Remove the tweet id and timestamp\n",
        "            tweetsData[i] = tweetsData[i][50:]\n",
        "\n",
        "            # Remove any word that starts with the symbol @\n",
        "            tweetsData[i] = \" \".join(filter(lambda x: x[0] != '@', tweetsData[i].split()))\n",
        "\n",
        "            # Remove any URL\n",
        "            tweetsData[i] = re.sub(r\"http\\S+\", \"\", tweetsData[i])\n",
        "            tweetsData[i] = re.sub(r\"www\\S+\", \"\", tweetsData[i])\n",
        "\n",
        "            # remove colons from the end of the sentences (if any) after removing url\n",
        "            tweetsData[i] = tweetsData[i].strip()\n",
        "            tweet_len = len(tweetsData[i])\n",
        "            if tweet_len > 0:\n",
        "                if tweetsData[i][len(tweetsData[i]) - 1] == ':':\n",
        "                    tweetsData[i] = tweetsData[i][:len(tweetsData[i]) - 1]\n",
        "\n",
        "            # Remove any hash-tags symbols\n",
        "            tweetsData[i] = tweetsData[i].replace('#', '')\n",
        "\n",
        "            # Remove any ? symbols\n",
        "            tweetsData[i] = tweetsData[i].replace('?', '')\n",
        "\n",
        "            # Convert every word to lowercase\n",
        "            tweetsData[i] = tweetsData[i].lower()\n",
        "\n",
        "            # remove punctuations\n",
        "            tweetsData[i] = tweetsData[i].translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "            # trim extra spaces\n",
        "            tweetsData[i] = \" \".join(tweetsData[i].split())\n",
        "\n",
        "            # convert each tweet from string type to as list<string> using \" \" as a delimiter\n",
        "            tweets_array.append(tweetsData[i].split(' '))\n",
        "\n",
        "        dataFile.close()\n",
        "\n",
        "        return tweets_array\n",
        "\n",
        "    def update_centroids(self, clusters):\n",
        "        updated_centroids = []\n",
        "        for i in range(len(clusters)):\n",
        "            min_dis = math.inf\n",
        "            new_cent = []\n",
        "            for j in range(len(clusters[i])):\n",
        "                candidate_cent = clusters[i][j][0]\n",
        "                cand_dis = 0\n",
        "                for k in range(len(clusters[i])):\n",
        "                    if j!=k:\n",
        "                        dis = self.jaccard_distance_calculation(candidate_cent,clusters[i][k][0])\n",
        "                        cand_dis = cand_dis + dis\n",
        "                if(cand_dis<min_dis):\n",
        "                    min_dis = cand_dis\n",
        "                    new_cent = candidate_cent\n",
        "            updated_centroids.append(new_cent)\n",
        "        \n",
        "        return updated_centroids\n",
        "\n",
        "    def main_kmeans(self, tweetsData):\n",
        "\n",
        "        prev_centroids = []\n",
        "        curr_centroids = []\n",
        "        # initialization, assign random tweets as centroids\n",
        "        count = 0\n",
        "        hash_map = dict()\n",
        "        while count < self.k:\n",
        "            random_tweet_idx = rd.randint(0, len(tweetsData) - 1)\n",
        "            if random_tweet_idx not in hash_map:\n",
        "                count += 1\n",
        "                hash_map[random_tweet_idx] = True\n",
        "                curr_centroids.append(tweetsData[random_tweet_idx])\n",
        "        \n",
        "        loop_count = 0\n",
        "        \n",
        "        # run the iterations until not converged or loop is not reached to the max iteration\n",
        "        while (self.is_converged(prev_centroids, curr_centroids)) == False and (loop_count < self.maxIter):\n",
        "\n",
        "            # assignment of each tweets to the closest centroids\n",
        "            clusters = self.assign_cluster_to_nearest_centroid(tweetsData, curr_centroids)\n",
        "\n",
        "            # we need to keep track of previous centroid for convergence check\n",
        "            prev_centroids = curr_centroids\n",
        "\n",
        "            # update centroid based on clusters formed\n",
        "            curr_centroids = self.update_centroids(clusters)\n",
        "            loop_count = loop_count + 1\n",
        "\n",
        "        s2_error = self.squared_error(clusters)\n",
        "        #Please do not play with the below few lines of code\n",
        "        count = 0\n",
        "        strInfo = ''\n",
        "        while count < self.k:\n",
        "            if count == 0:\n",
        "                strInfo = strInfo + str(count+1)+': '+str(len(clusters[count]))+' tweets \\n'\n",
        "            else:\n",
        "                strInfo = strInfo + \"                    \" + str(count+1)+': '+str(len(clusters[count]))+' tweets \\n'\n",
        "            \n",
        "            count = count + 1\n",
        "        print(\"   \"+str(self.k)+\"        \"+str(s2_error)+\"    \"+strInfo)\n",
        "        \n",
        "        return clusters, s2_error\n",
        "\n",
        "    def is_converged(self, prev_centroid, new_centroids):\n",
        "        # if lengths are not equal then funtion will return false\n",
        "        if len(prev_centroid) != len(new_centroids):\n",
        "            return False\n",
        "\n",
        "        # iterate over each entry of clusters and check if they are same\n",
        "        for i in range(len(new_centroids)):\n",
        "            if \" \".join(new_centroids[i]) != \" \".join(prev_centroid[i]):\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    # Jaccard Distance calculator\n",
        "    def jaccard_distance_calculation(self, tweet1, tweet2):\n",
        "        # Jaccard distance is: 1 - length(tweet1 intersection tweet2)/length(tweet1 union tweets2)\n",
        "        # get the intersection\n",
        "        intersectionLength = len(set(tweet1).intersection(tweet2))\n",
        "\n",
        "        # get the union\n",
        "        unionLength = len(set().union(tweet1, tweet2))\n",
        "\n",
        "        # return the jaccard distance\n",
        "        return round(1 - (float(intersectionLength) / unionLength), 5)\n",
        "\t\t\n",
        "\n",
        "    def assign_cluster_to_nearest_centroid(self, tweetsData, centroids):\n",
        "        clusters = dict()\n",
        "\n",
        "        # for every tweet iterate each centroid and assign closest centroid to a it\n",
        "        for i in range(len(tweetsData)):\n",
        "            min_dis = math.inf\n",
        "            cluster_idx = -1;\n",
        "            for j in range(len(centroids)):\n",
        "                dis = self.jaccard_distance_calculation(centroids[j], tweetsData[i])\n",
        "                \n",
        "                if centroids[j] == tweetsData[i]:\n",
        "                    cluster_idx = j\n",
        "                    min_dis = 0\n",
        "                    break\n",
        "\n",
        "                if dis < min_dis:\n",
        "                    cluster_idx = j\n",
        "                    min_dis = dis\n",
        "\n",
        "            # if minimum distance = 1 i.e. the there is no word common between centroid and the current tweet, assign any random cluster\n",
        "            if min_dis == 1:\n",
        "                cluster_idx = rd.randint(0, len(centroids) - 1)\n",
        "\n",
        "            # centroid allocation to a tweet\n",
        "            clusters.setdefault(cluster_idx, []).append([tweetsData[i]])\n",
        "\n",
        "            # Distance of each tweet form closest centroid, this min_dis will be used for squared error calculation\n",
        "            last_tweet_idx = len(clusters.setdefault(cluster_idx, [])) - 1\n",
        "            clusters.setdefault(cluster_idx, [])[last_tweet_idx].append(min_dis)\n",
        "        #print(str(clusters.setdefault(cluster_idx, []))+': '+str(len(clusters.setdefault(cluster_idx, [])))+' tweets')\n",
        "        return clusters\n",
        "\n",
        "    def squared_error(self, clusters):\n",
        "        squared_error = 0\n",
        "        # For every cluster squared_error is calculated as the sum of square of distances of the tweet from the associated centroid\n",
        "        for i in range(len(clusters)):\n",
        "            for j in range(len(clusters[i])):\n",
        "                squared_error = squared_error + int(pow(clusters[i][j][1], 2))\n",
        "            \n",
        "        return squared_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol_pBkzSs_-S"
      },
      "outputs": [],
      "source": [
        "strURL = 'https://raw.githubusercontent.com/lordphoenix/CS6375/main/cbchealth.txt'\n",
        "squared_error = []\n",
        "k = 0\n",
        "tolerance = 0.001\n",
        "maxIter = 10\n",
        "kMeanRunCount = 10\n",
        "\n",
        "print('Value of K  '+' SSE '+'  Size of each cluster')\n",
        "data_table = []\n",
        "s2error = []\n",
        "for i in range(kMeanRunCount):\n",
        "    k = k + 5\n",
        "    \n",
        "    kMeans = K_Means_Class(strURL, k, tolerance, maxIter, data_table)\n",
        "    tweetsData = kMeans.pre_process_raw_data()\n",
        "    kCentroid, s2_error = kMeans.main_kmeans(tweetsData)\n",
        "    s2error.append(s2_error)\n",
        "\n",
        "\n",
        "sns.lineplot(x=range(0,kMeanRunCount), y=s2error, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.title('The Elbow Method Graph')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS6375_Assignment3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
