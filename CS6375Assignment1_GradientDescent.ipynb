{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb3h2_Scs9mJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSOITE15tCMB"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self,num_iter,strUrl,strDelim,tolerance):\n",
        "        #constructor\n",
        "        self.num_iter = num_iter\n",
        "        self.strUrl = strUrl\n",
        "        self.strDelim = strDelim\n",
        "        self.tolerance = tolerance\n",
        "        \n",
        "    def data_preprocessing(self):\n",
        "\n",
        "        #Read the CSV file from Google Drive and create the data frame\n",
        "        dataFrame = pd.read_csv('https://drive.google.com/uc?export=download&id=' +self.strUrl.split('/')[-2], encoding = 'unicode_escape', delimiter = self.strDelim)\n",
        "\n",
        "        # To drop any blank rows\n",
        "        dataFrame.dropna(axis = 0, how = 'any', thresh = None, inplace = True) \n",
        "\n",
        "        # Drop duplicate rows\n",
        "        dataFrame.drop_duplicates(inplace=True)\n",
        "\n",
        "        #Rebuild index after dropping rows\n",
        "        dataFrame.reset_index(drop = True, inplace = True)\n",
        "\n",
        "        # Convert Functioning Day value as 'Yes': 1, 'No' : 0\n",
        "        dataFrame['Functioning Day'] = dataFrame['Functioning Day'].map({'Yes': 1, 'No' : 0})\n",
        "\n",
        "        # Convert Holiday value as 'Holiday': 1, 'No Holiday' : 0\n",
        "        dataFrame['Holiday'] = dataFrame['Holiday'].map({'Holiday': 1, 'No Holiday' : 0})\n",
        "\n",
        "        # Convert Seasons values as 'Winter' : 1, 'Spring' : 2, 'Summer' : 3, 'Autumn' : 4\n",
        "        dataFrame['Seasons'] = dataFrame['Seasons'].map({'Winter' : 1, 'Spring' : 2, 'Summer' : 3, 'Autumn' : 4})\n",
        "\n",
        "        X = dataFrame.iloc[:, [2, 3, 4, 5, 6, 7, 8]]\n",
        "\n",
        "        Y = dataFrame.iloc[:, [1]]\n",
        "        scalar = StandardScaler()\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
        "\n",
        "        # scaling the values\n",
        "        X_train = scalar.fit(X_train).fit_transform(X_train)\n",
        "        Y_train = scalar.fit(Y_train).fit_transform(Y_train)\n",
        "        \n",
        "        X_test = scalar.fit(X_test).fit_transform(X_test)\n",
        "        Y_test = scalar.fit(Y_test).fit_transform(Y_test)\n",
        "\n",
        "        #Adding a column of 1 in the beginning\n",
        "        X_train = np.hstack((np.ones((len(X_train),1)),X_train))\n",
        "        X_test = np.hstack((np.ones((len(X_test),1)),X_test))\n",
        "        \n",
        "        return X_train,Y_train,X_test,Y_test\n",
        "    \n",
        "    def compute_cost(self,X,y):\n",
        "        #Calculating error on formulae : Err = [Sum((Hx(theta)-Y_expected)^2)]/(2*number_of_training_examples)\n",
        "        y1 = np.matmul(X,self.theta)\n",
        "        err = np.subtract(y1,y)\n",
        "        return (np.sum(np.square(err)))/(2*len(X))\n",
        "\n",
        "    def compute_variance(self, X_test, Y_test):\n",
        "        Y_predict = np.matmul(X_test,self.theta)\n",
        "        err = np.subtract(Y_predict,Y_test)\n",
        "        return (np.var(err))\n",
        "        \n",
        "    def is_converged(self,oldTheta):\n",
        "        #Returns true if difference between new weights and old weights is less than tolerance value\n",
        "        diff = np.absolute(np.subtract(self.theta,oldTheta))\n",
        "        meanDiff = np.mean(diff)\n",
        "        return meanDiff < self.tolerance\n",
        "\n",
        "    def gradient_descent(self,X_train,y_train,alpha):\n",
        "        hist_cost = []\n",
        "        hist_theta = []\n",
        "\n",
        "        #initializing theta values with 0 shape : (number_of_attributes+1,1)\n",
        "        self.theta = np.zeros((len(X_train[0]),1))\n",
        "\n",
        "        n_iter = 0\n",
        "\n",
        "        #iterating num_iter times and adjusting weights\n",
        "        for i in range(0,self.num_iter):  \n",
        "            #saving old theta to check convergance\n",
        "            oldTheta = self.theta\n",
        "            #saving old theta values\n",
        "            hist_theta.append(self.theta) \n",
        "            #calculating hypothesis\n",
        "            y1 = np.matmul(X_train,self.theta)\n",
        "            #calculating gradient \n",
        "            gradient = (np.matmul(X_train.transpose(),(y1-y_train)))/len(X_train)\n",
        "            #updating weights\n",
        "            self.theta = self.theta - (alpha*gradient)\n",
        "            #saving the costs for analysis\n",
        "            hist_cost.append(self.compute_cost(X_train,y_train))\n",
        "            #breaking the loop if converged\n",
        "            n_iter = n_iter + 1\n",
        "            if self.is_converged(oldTheta):\n",
        "                break\n",
        "        #returning historical cost and theta values\n",
        "        return hist_cost,hist_theta,n_iter\n",
        "\n",
        "    def eval_model(self,X_test,y_test):\n",
        "        #calculating root mean squared error\n",
        "        cost_test = np.sqrt(2*self.compute_cost(X_test,y_test))\n",
        "        #returning y_predicted\n",
        "        y_predict = np.matmul(X_test,self.theta)\n",
        "        variance_val = self.compute_variance(X_test,y_test)\n",
        "        return cost_test,y_predict,variance_val\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    datasetUrl = 'https://drive.google.com/file/d/1M9KEyuwehbqOur2CKwN8wQL7DJvMhFTX/view?usp=sharing'\n",
        "    delimeter = ','\n",
        "\n",
        "    tolerance = 0.00001\n",
        "    num_iter = 5000\n",
        "\n",
        "    #creating a object to which will help to find minimal alpha\n",
        "    model = LinearRegression(num_iter,datasetUrl,delimeter,tolerance)\n",
        "    #preprocessing data\n",
        "    X_train,y_train,X_test,y_test = model.data_preprocessing()\n",
        "    #to store historical values\n",
        "    hist_alpha = []\n",
        "    hist_error = []\n",
        "    #minimum alpha\n",
        "    res_alpha = 0.0\n",
        "    #minimum error - declaring it as large value for better comparison\n",
        "    min_error = 1e9\n",
        "\n",
        "    #checking for 100 iterations\n",
        "    for i in range(1, 300):\n",
        "        #obtaining small alpha by dividing it by 1000\n",
        "        alpha = i/1000\n",
        "        #using gradient descent with the above specified alpha\n",
        "        model.gradient_descent(X_train,y_train,alpha)\n",
        "        #saving alpha to use as X-axis later\n",
        "        hist_alpha.append(alpha)\n",
        "        #if error computed is smaller than min_error, update both min_error and res_alpha\n",
        "        if model.compute_cost(X_test,y_test) < min_error:\n",
        "            min_error = model.compute_cost(X_test,y_test)\n",
        "            res_alpha = alpha\n",
        "        #saving historical error values to plot\n",
        "        hist_error.append(model.compute_cost(X_test,y_test))\n",
        "\n",
        "    print(\"Min Alpha: \",res_alpha)\n",
        "    print('\\n')\n",
        "    #plotting alpha vs all error values to see the trend\n",
        "    plt.plot(hist_alpha,hist_error)\n",
        "    plt.title('Alpha vs Error for Trial')\n",
        "    plt.xlabel('Alpha')\n",
        "    plt.ylabel('Error')\n",
        "    plt.show()\n",
        "    \n",
        "    #performing gradient descent for res_alpha\n",
        "    hist_cost,hist_theta,total_iter = model.gradient_descent(X_train,y_train,res_alpha)\n",
        "    \n",
        "    #evaluating model and fetching rmse and predicated output value\n",
        "    rmse,y_predict,variance_val = model.eval_model(X_test,y_test)\n",
        "\n",
        "    #calculating r2_value\n",
        "    r2_value = r2_score(y_test,y_predict)\n",
        "    \n",
        "    #output\n",
        "    print('\\nRMSE: ',rmse)\n",
        "    print('\\nR2 Value: ',r2_value)\n",
        "    print('\\nVariance Value: ',variance_val)\n",
        "    print('\\n')\n",
        "    print('Total Iteration: ',total_iter)\n",
        "    print('\\n')\n",
        "    \n",
        "    plt.plot(hist_cost)\n",
        "    plt.title('Error vs Iteration Alpha = ' + str(res_alpha))\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Error')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hc2QAQbo6je"
      },
      "outputs": [],
      "source": [
        "#Trials\n",
        "\n",
        "dis_alpha = [0.0001,\n",
        "0.0002,\n",
        "0.001,\n",
        "0.002,\n",
        "0.01,\n",
        "0.1,\n",
        "0.15,\n",
        "0.25,\n",
        "0.3,\n",
        "0.8]\n",
        "\n",
        "dis_tolerance = [0.0000001,\n",
        "0.0000001,\n",
        "0.000001,\n",
        "0.00001,\n",
        "0.0001,\n",
        "0.001,\n",
        "0.001,\n",
        "0.001,\n",
        "0.001,\n",
        "0.001]\n",
        "\n",
        "datasetUrl = 'https://drive.google.com/file/d/1M9KEyuwehbqOur2CKwN8wQL7DJvMhFTX/view?usp=sharing'\n",
        "delimeter = ','\n",
        "max_iter = 10000\n",
        "\n",
        "hist_total_iter = []\n",
        "hist_rmse = []\n",
        "hist_r2 = []\n",
        "hist_variance = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    model = LinearRegression(max_iter,datasetUrl,delimeter,dis_tolerance[i])\n",
        "    X_train,y_train,X_test,y_test = model.data_preprocessing()\n",
        "    hist_cost,hist_theta,total_iter = model.gradient_descent(X_train,y_train,dis_alpha[i])\n",
        "    rmse,y_predict,variance_val = model.eval_model(X_test,y_test)\n",
        "    r2_value = r2_score(y_test,y_predict)\n",
        "    hist_total_iter.append(total_iter)\n",
        "    hist_rmse.append(rmse)\n",
        "    hist_r2.append(r2_value)\n",
        "    hist_variance.append(variance_val)\n",
        "\n",
        "print(\"\\n RMSE Values : \")\n",
        "print(hist_rmse)\n",
        "print(\"\\n R2 Values : \")\n",
        "print(hist_r2)\n",
        "print(\"\\nIteration Values : \")\n",
        "print(hist_total_iter)\n",
        "print(\"\\nVariance values : \")\n",
        "print(hist_variance)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS6375Assignment1-GradientDescent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
